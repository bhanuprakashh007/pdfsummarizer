import os
import streamlit as st
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_community.document_loaders import PyPDFLoader
from langchain_core.prompts import PromptTemplate
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.chains.summarize import load_summarize_chain
from dotenv import load_dotenv

# Load environment variables from API.env file
load_dotenv('API.env')

def get_llm():
    """
    Instantiates and returns the LLM model.
    """
    api_key = os.getenv("GEMINI_API_KEY")
    if not api_key:
        st.error("GEMINI_API_KEY not found. Please set it in your API.env file.")
        return None

    llm = ChatGoogleGenerativeAI(
        model="gemini-1.5-flash-latest",
        temperature=0.3,
        google_api_key=api_key
    )
    return llm

def process_pdf(pdf_file_path, custom_prompt_text, chain_type, chunk_size, chunk_overlap):
    """
    Loads, splits, and summarizes a PDF based on the provided parameters.
    """
    llm = get_llm()
    if not llm:
        return None

    # Load and split the PDF using dynamic chunk parameters
    loader = PyPDFLoader(pdf_file_path)
    docs_chunks = loader.load_and_split(
        text_splitter=RecursiveCharacterTextSplitter(
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap
        )
    )

    # Create and run the summarization chain based on the selected type
    if chain_type == "stuff":
        # The stuff method uses a single prompt for the entire document
        prompt_template = custom_prompt_text + """

        {text}

        """
        prompt = PromptTemplate.from_template(prompt_template)
        chain = load_summarize_chain(llm, chain_type="stuff", prompt=prompt)
        
    elif chain_type == "map_reduce":
        # The map_reduce method summarizes chunks first, then combines them
        map_prompt_template = "Summarize the following text chunk concisely:\n\n{text}"
        map_prompt = PromptTemplate.from_template(map_prompt_template)

        combine_prompt_template = custom_prompt_text + """

        {text}
        """
        combine_prompt = PromptTemplate.from_template(combine_prompt_template)
        
        chain = load_summarize_chain(
            llm,
            chain_type="map_reduce",
            map_prompt=map_prompt,
            combine_prompt=combine_prompt,
            verbose=True
        )
    else:
        st.error("Invalid chain type selected.")
        return None

    result = chain.invoke({"input_documents": docs_chunks})
    return result['output_text']

def main():
    """
    The main function to run the Streamlit application.
    """
    st.set_page_config(page_title="PDF Summarizer", page_icon="üìù", layout="wide")
    
    st.title("PDF Summarizer with Gemini")
    st.markdown("This application allows you to upload a PDF file, provide a custom instruction (a prompt), and get a tailored summary generated by the Gemini model.")

    # Sidebar for controls
    with st.sidebar:
        st.header("Settings")
        
        # Dropdown to select the summarization method
        chain_type = st.selectbox(
            "Summarization Method",
            ("stuff", "map_reduce"),
            help="**Stuff**: Faster, but may fail on very large documents. **MapReduce**: Slower, but handles large documents by summarizing chunks first."
        )

        if chain_type == "map_reduce":
            st.warning("The 'MapReduce' method can take significantly longer to process.")

        # Sliders for chunk size and overlap
        chunk_size = st.slider(
            "Chunk Size",
            min_value=500,
            max_value=20000,
            value=4000,
            step=500,
            help="The maximum number of characters in each text chunk."
        )
        chunk_overlap = st.slider(
            "Chunk Overlap",
            min_value=0,
            max_value=5000,
            value=200,
            step=50,
            help="The number of characters to overlap between adjacent chunks."
        )

    # Main application area
    st.markdown("---")
    st.subheader("1. Upload your PDF file")
    uploaded_file = st.file_uploader("", type="pdf")

    if uploaded_file is not None:
        # Save the uploaded file to a temporary location
        temp_file_path = "temp_uploaded_file.pdf"
        with open(temp_file_path, "wb") as f:
            f.write(uploaded_file.getbuffer())

        st.success(f"Successfully uploaded `{uploaded_file.name}`")

        st.subheader("2. Enter your custom prompt")
        custom_prompt = st.text_area(
            "Custom Prompt",
            height=150,
            placeholder="For example: 'Summarize the key findings of this research paper for a non-technical audience in five bullet points.'"
        )

        if st.button("Generate Summary", type="primary"):
            if not custom_prompt:
                st.warning("Please enter a prompt to guide the summary.")
            else:
                with st.spinner("Generating summary... This may take a moment."):
                    try:
                        summary = process_pdf(
                            temp_file_path,
                            custom_prompt,
                            chain_type,
                            chunk_size,
                            chunk_overlap
                        )
                        if summary:
                            st.subheader("Custom Summary")
                            st.success(summary)
                    except Exception as e:
                        st.error(f"An error occurred: {e}")
        
        # Clean up the temporary file after processing
        if os.path.exists(temp_file_path):
            os.remove(temp_file_path)

if __name__ == "__main__":
    main()
